{
    "columns": [
        {
            "accessor": "model",
            "name": "Model",
            "abbr": "Model"
        },
        {
            "accessor": "release_date",
            "name": "Release Date",
            "abbr": "Released"
        },
        {
            "accessor": "context_length",
            "name": "Context Length",
            "abbr": "Context",
            "description": "Context Length, maximum number of tokens allowed as input"
        },
        {
            "accessor": "vocabulary_size",
            "name": "Vocabulary Size",
            "abbr": "Vocab Size",
            "description": "Vocabulary size, number of unique tokens handled by the model"
        },
        {
            "accessor": "nb_parameters",
            "name": "Parameters Count",
            "abbr": "Nb Params",
            "description": "Number of trainable model parameters, in billion"
        },
        {
            "accessor": "nb_layers",
            "name": "Number of Layers",
            "abbr": "N",
            "description": "Number of layers used in encoder/decoder",
            "isMath": true,
            "abbrMath": "N"
        },
        {
            "accessor": "nb_dimensions",
            "name": "Hidden Dimensions",
            "abbr": "d_model",
            "description": "Embedding dimension",
            "isMath": true,
            "abbrMath": "d_{\\textrm{model}}"
        },
        {
            "accessor": "nb_heads",
            "name": "Attention Heads",
            "abbr": "h",
            "description": "Number of attention heads",
            "isMath": true,
            "abbrMath": "h"
        },
        {
            "accessor": "nb_ffn_layers",
            "name": "Feed Forward Layers",
            "abbr": "d_ff",
            "description": "Dimension of the feed-forward layer, when applicable",
            "isMath": true,
            "abbrMath": "d_{\\textrm{ff}}"
        },
        {
            "accessor": "nb_tokens",
            "name": "Training Tokens",
            "abbr": "Tokens",
            "description": "Number of tokens used for training, in trillion"
        },
        {
            "accessor": "activation",
            "name": "Activation",
            "abbr": "Activation"
        },
        {
            "accessor": "gqa",
            "name": "Group Query Attention",
            "abbr": "GQA",
            "description": "Used Group Query Attention"
        },
        {
            "accessor": "hardware",
            "name": "Hardware",
            "abbr": "Hardware"
        },
        {
            "accessor": "gpu_power",
            "name": "GPU Power (W)",
            "abbr": "GPU Power (W)"
        },
        {
            "accessor": "gpu_hours",
            "name": "GPU Hours",
            "abbr": "GPU Hours"
        },
        {
            "accessor": "tco2eq",
            "name": "Carbon Footprint (tCO2eq)",
            "abbr": "tC02eq",
            "description": "Carbon footprint measured in ton of CO2 equivalent (tCO2eq)"
        }
    ],
    "models": [
        {
            "family": "gpt-1",
            "company": "openai",
            "description": "Improving language understanding with unsupervised learning",
            "urlPaper": "https://openai.com/research/language-unsupervised",
            "source": "openai",
            "releaseDate": "2018-06-11"
        },
        {
            "family": "bert",
            "company": "google",
            "description": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "urlPaper": "https://arxiv.org/abs/1810.04805",
            "source": "arxiv",
            "releaseDate": "2018-10-11"
        },
        {
            "family": "gpt-2",
            "company": "openai",
            "description": "Better language models and their implications",
            "urlPaper": "https://openai.com/research/better-language-models",
            "source": "openai",
            "releaseDate": "2019-02-14"
        },
        {
            "family": "gpt-3",
            "company": "openai",
            "description": "Language Models are Few-Shot Learners",
            "urlPaper": "https://arxiv.org/abs/2005.14165",
            "source": "openai",
            "releaseDate": "2020-05-28"
        },
        {
            "family": "opt",
            "company": "meta",
            "description": "OPT: Open Pre-trained Transformer Language Models",
            "urlPaper": "https://arxiv.org/abs/2205.01068",
            "source": "arxiv",
            "releaseDate": "2022-06-21"
        },
        {
            "family": "palm",
            "company": "google",
            "description": "PaLM: Scaling Language Modeling with Pathways",
            "urlPaper": "https://arxiv.org/abs/2204.02311",
            "source": "arxiv",
            "releaseDate": "2022-10-05"
        },
        {
            "family": "llama-1",
            "company": "meta",
            "description": "LLaMA: Open and Efficient Foundation Language Models",
            "urlPaper": "https://arxiv.org/abs/2302.13971",
            "source": "arxiv",
            "releaseDate": "2023-02-27"
        },
        {
            "family": "llama-2",
            "company": "meta",
            "description": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "urlPaper": "https://arxiv.org/abs/2307.09288",
            "source": "arxiv",
            "releaseDate": "2023-07-18"
        },
        {
            "family": "mistral-7b",
            "company": "mistral-ai",
            "description": "Mistral 7B",
            "urlPaper": "https://arxiv.org/abs/2310.06825",
            "source": "arxiv",
            "releaseDate": "2023-10-10"
        },
        {
            "family": "mixtral-8x7b",
            "company": "mistral-ai",
            "description": "Mixtral of Experts",
            "urlPaper": "https://arxiv.org/abs/2401.04088",
            "source": "arxiv",
            "releaseDate": "2024-01-08"
        },
        {
            "family": "gemma",
            "company": "google",
            "description": "Gemma: Open Models Based on Gemini Research and Technology",
            "urlPaper": "https://arxiv.org/abs/2403.08295",
            "source": "arxiv",
            "releaseDate": "2024-03-13"
        },
        {
            "family": "mixtral-8x22b",
            "company": "mistral-ai",
            "description": "Cheaper, Better, Faster, Stronger",
            "urlPaper": "https://mistral.ai/news/mixtral-8x22b/",
            "source": "arxiv",
            "releaseDate": "2024-04-17"
        },
        {
            "family": "phi-3",
            "company": "microsoft",
            "description": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "urlPaper": "https://arxiv.org/abs/2404.14219",
            "source": "arxiv",
            "releaseDate": "2024-04-23"
        },
        {
            "family": "openelm",
            "company": "apple",
            "description": "OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework",
            "urlPaper": "https://arxiv.org/abs/2404.14619",
            "source": "arxiv",
            "releaseDate": "2024-05-02"
        },
        {
            "family": "llama-3",
            "company": "meta",
            "description": "Introducing Meta Llama 3: The most capable openly available LLM to date",
            "urlPaper": "https://ai.meta.com/blog/meta-llama-3/",
            "source": "meta",
            "releaseDate": "2024-04-18"
        },
        {
            "family": "llama-3.1",
            "company": "meta",
            "description": "The Llama 3 Herd of Models",
            "urlPaper": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
            "source": "meta",
            "releaseDate": "2024-07-23"
        },
        {
            "family": "gemini-1.0",
            "company": "google",
            "description": "Gemini: A Family of Highly Capable Multimodal Models",
            "urlPaper": "https://arxiv.org/abs/2403.05530",
            "source": "arxiv",
            "releaseDate": "2023-12-19"
        },
        {
            "family": "gemini-1.5",
            "company": "google",
            "description": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "urlPaper": "https://arxiv.org/abs/2403.05530",
            "source": "arxiv",
            "releaseDate": "2024-03-08"
        },
        {
            "family": "gpt-4",
            "company": "openai",
            "description": "GPT-4 Technical Report",
            "urlPaper": "https://arxiv.org/abs/2303.08774",
            "source": "arxiv",
            "releaseDate": "2020-03-14"
        },
        {
            "family": "gpt-4o",
            "company": "openai",
            "description": "Hello GPT-4o",
            "urlPaper": "https://openai.com/index/hello-gpt-4o/",
            "source": "openai",
            "releaseDate": "2020-05-13"
        },
        {
            "family": "gpt-4o-mini",
            "company": "openai",
            "description": "GPT-4o mini: advancing cost-efficient intelligence",
            "urlPaper": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
            "source": "openai",
            "releaseDate": "2020-07-18"
        },
        {
            "family": "claude-3.5",
            "company": "anthropic",
            "description": "Claude 3.5 Sonnet",
            "urlPaper": "https://www.anthropic.com/news/claude-3-5-sonnet",
            "source": "anthropic",
            "releaseDate": "2024-06-21"
        },
        {
            "family": "claude-3",
            "company": "anthropic",
            "description": "Introducing the next generation of Claude",
            "urlPaper": "https://www.anthropic.com/news/claude-3-family",
            "source": "anthropic",
            "releaseDate": "2024-03-04"
        },
        {
            "family": "claude-2.1",
            "company": "anthropic",
            "description": "Introducing Claude 2.1",
            "urlPaper": "https://www.anthropic.com/news/claude-2-1",
            "source": "anthropic",
            "releaseDate": "2023-11-21"
        },
        {
            
            "family": "claude-2",
            "company": "anthropic",
            "description": "Claude 2",
            "urlPaper": "https://www.anthropic.com/news/claude-2",
            "source": "anthropic",
            "releaseDate": "2023-07-11"
        }
    ],
    "activations": [
        {
            "name": "ReLU",
            "formula": "\\textrm{ReLU}(x) = \\max(0, x)"
        },
        {
            "name": "SwiGLU",
            "formula": "\\textrm{SwiGLU}(x) = \\max(0, x) + \\sigma(x) \\min(0, x)"
        },
        {
            "name": "NewGELU",
            "formula": "\\textrm{NewGELU}(x) = 0.5 x (1 + \\tanh ( \\sqrt{2/\\pi}(x + 0.044715 x^3)))"
        },
        {
            "name": "GELU",
            "formula": "\\textrm{GELU}(x) = 0.5 x (1 + \\textrm{erf} (x / \\sqrt{2}))"
        },
        {
            "name": "SiLU",
            "formula": "\\textrm{SiLU}(x) = x \\sigma(x)"
        },
        {
            "name": "GeGLU",
            "formula": "\\textrm{GeGLU}(x, W, V, b, c) = \\textrm{GELU}(xW+b) \\otimes (x V + c)"
        }
    ]
}