[
  {
    "model": "openelm-270M",
    "release_date": "2024-04-22",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 0.27,
    "nb_layers": 16,
    "nb_dimensions": 1280,
    "nb_heads": 10,
    "nb_tokens": 1.8,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 9216.0
  },
  {
    "model": "openelm-450M",
    "release_date": "2024-04-22",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 0.45,
    "nb_layers": 20,
    "nb_dimensions": 1536,
    "nb_heads": 12,
    "nb_tokens": 1.8,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "H100-80GB",
    "gpu_hours": 9216.0
  },
  {
    "model": "openelm-1.1B",
    "release_date": "2024-04-22",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 1.1,
    "nb_layers": 28,
    "nb_dimensions": 2048,
    "nb_heads": 16,
    "nb_tokens": 1.8,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 33792.0
  },
  {
    "model": "openelm-3B",
    "release_date": "2024-04-22",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 3.0,
    "nb_layers": 36,
    "nb_dimensions": 3072,
    "nb_heads": 24,
    "nb_tokens": 1.8,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "H100-80GB",
    "gpu_hours": 39936.0
  },
  {
    "model": "phi-3-mini",
    "release_date": "2024-04-22",
    "context_length": 4096,
    "vocabulary_size": 32064,
    "nb_parameters": 3.8,
    "nb_layers": 32,
    "nb_dimensions": 3072,
    "nb_heads": 32,
    "nb_ffn_layers": 8192,
    "nb_tokens": 3.3,
    "gqa": false,
    "activation": "SiLU"
  },
  {
    "model": "phi-3-mini-128K",
    "release_date": "2024-04-22",
    "context_length": 128000,
    "vocabulary_size": 32064,
    "nb_parameters": 3.8,
    "nb_layers": 32,
    "nb_dimensions": 3072,
    "nb_heads": 32,
    "nb_ffn_layers": 8192,
    "nb_tokens": 3.3,
    "gqa": false,
    "activation": "SiLU"
  },
  {
    "model": "phi-3-small",
    "release_date": "2024-04-22",
    "context_length": 8192,
    "vocabulary_size": 100352,
    "nb_parameters": 7.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_tokens": 4.8,
    "gqa": true,
    "activation": "SiLU"
  },
  {
    "model": "phi-3-medium",
    "release_date": "2024-04-22",
    "context_length": 8192,
    "vocabulary_size": 100352,
    "nb_parameters": 14.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_tokens": 4.8,
    "gqa": true,
    "activation": "SiLU"
  },
  {
    "model": "llama-3-8b",
    "release_date": "2024-04-18",
    "context_length": 8192,
    "vocabulary_size": 128256,
    "nb_parameters": 8.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "nb_tokens": 15.0,
    "gqa": true
  },
  {
    "model": "llama-3-70b",
    "release_date": "2024-04-18",
    "context_length": 8192,
    "vocabulary_size": 128256,
    "nb_parameters": 70.0,
    "nb_layers": 64,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 15.0,
    "gqa": true
  },
  {
    "model": "mixtral-8x22b",
    "release_date": "2024-04-17",
    "context_length": 65536,
    "vocabulary_size": 32768,
    "nb_parameters": 176.0,
    "nb_layers": 56,
    "nb_dimensions": 6144,
    "nb_heads": 48,
    "nb_ffn_layers": 16384,
    "gqa": true,
    "activation": "SiLU"
  },
  {
    "model": "gemma-2b",
    "release_date": "2024-03-13",
    "context_length": 8192,
    "vocabulary_size": 256000,
    "nb_parameters": 2.0,
    "nb_layers": 18,
    "nb_dimensions": 2048,
    "nb_heads": 8,
    "nb_ffn_layers": 32768,
    "nb_tokens": 3.0,
    "gqa": false,
    "activation": "GeGLU"
  },
  {
    "model": "gemma-7b",
    "release_date": "2024-03-13",
    "context_length": 8192,
    "vocabulary_size": 256000,
    "nb_parameters": 7.0,
    "nb_layers": 28,
    "nb_dimensions": 3072,
    "nb_heads": 16,
    "nb_ffn_layers": 49152,
    "nb_tokens": 6.0,
    "gqa": false,
    "activation": "GeGLU"
  },
  {
    "model": "mixtral-8x7b",
    "release_date": "2024-01-08",
    "context_length": 32768,
    "vocabulary_size": 32000,
    "nb_parameters": 56.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "gqa": true,
    "activation": "SiLU"
  },
  {
    "model": "mistral-7b",
    "release_date": "2023-10-10",
    "context_length": 8192,
    "vocabulary_size": 32000,
    "nb_parameters": 7.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "nb_tokens": 8.0,
    "gqa": false,
    "activation": "SiLU"
  },
  {
    "model": "llama-2-7b",
    "release_date": "2023-07-18",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 7.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 11008,
    "nb_tokens": 2.0,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 184320.0,
    "gpu_power": 400.0,
    "tco2eq": 31.22
  },
  {
    "model": "llama-2-13b",
    "release_date": "2023-07-18",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": 13824,
    "nb_tokens": 2.0,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 368640.0,
    "gpu_power": 400.0,
    "tco2eq": 62.44
  },
  {
    "model": "llama-2-34b",
    "release_date": "2023-07-18",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 34.0,
    "nb_layers": 60,
    "nb_dimensions": 6656,
    "nb_heads": 52,
    "nb_ffn_layers": 22016,
    "nb_tokens": 2.0,
    "gqa": true,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 1038336.0,
    "gpu_power": 350.0,
    "tco2eq": 153.9
  },
  {
    "model": "llama-2-70b",
    "release_date": "2023-07-18",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 70.0,
    "nb_layers": 80,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 2.0,
    "gqa": true,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 1720320.0,
    "gpu_power": 400.0,
    "tco2eq": 291.42
  },
  {
    "model": "llama-1-7b",
    "release_date": "2023-02-27",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 6.7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 11008,
    "nb_tokens": 1.0,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 82432.0,
    "gpu_power": 400.0,
    "tco2eq": 14.0,
    "total_mwh": 36.0
  },
  {
    "model": "llama-1-13b",
    "release_date": "2023-02-27",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": 13824,
    "nb_tokens": 1.0,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 135168.0,
    "gpu_power": 400.0,
    "tco2eq": 23.0,
    "total_mwh": 59.0
  },
  {
    "model": "llama-1-33b",
    "release_date": "2023-02-27",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 32.5,
    "nb_layers": 60,
    "nb_dimensions": 6656,
    "nb_heads": 52,
    "nb_ffn_layers": 22016,
    "nb_tokens": 1.4,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 530432.0,
    "gpu_power": 400.0,
    "tco2eq": 90.0,
    "total_mwh": 233.0
  },
  {
    "model": "llama-1-65b",
    "release_date": "2023-02-27",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 65.2,
    "nb_layers": 80,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 1.4,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "A100-80GB",
    "gpu_hours": 1022362.0,
    "gpu_power": 400.0,
    "tco2eq": 173.0,
    "total_mwh": 449.0
  },
  {
    "model": "palm-8B",
    "release_date": "2022-10-05",
    "nb_parameters": 8.63,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 16,
    "nb_ffn_layers": 16384,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "TPU v4"
  },
  {
    "model": "palm-62B",
    "release_date": "2022-10-05",
    "nb_parameters": 62.5,
    "nb_layers": 64,
    "nb_dimensions": 8192,
    "nb_heads": 32,
    "nb_ffn_layers": 32768,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "TPU v4"
  },
  {
    "model": "palm-540B",
    "release_date": "2022-10-05",
    "nb_parameters": 540.35,
    "nb_layers": 118,
    "nb_dimensions": 18432,
    "nb_heads": 48,
    "nb_ffn_layers": 73728,
    "gqa": false,
    "activation": "SwiGLU",
    "hardware": "TPU v4"
  },
  {
    "model": "opt-350M",
    "release_date": "2022-06-21",
    "nb_parameters": 0.35,
    "nb_layers": 24,
    "nb_dimensions": 1024,
    "nb_heads": 16,
    "nb_ffn_layers": 4096,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-1.3B",
    "release_date": "2022-06-21",
    "nb_parameters": 1.3,
    "nb_layers": 24,
    "nb_dimensions": 2048,
    "nb_heads": 32,
    "nb_ffn_layers": 8192,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-175B",
    "release_date": "2022-06-21",
    "nb_parameters": 175.0,
    "nb_layers": 96,
    "nb_dimensions": 12288,
    "nb_heads": 96,
    "nb_ffn_layers": 49152,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-66B",
    "release_date": "2022-06-21",
    "nb_parameters": 66.0,
    "nb_layers": 64,
    "nb_dimensions": 9216,
    "nb_heads": 72,
    "nb_ffn_layers": 36864,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-30B",
    "release_date": "2022-06-21",
    "nb_parameters": 30.0,
    "nb_layers": 48,
    "nb_dimensions": 7168,
    "nb_heads": 56,
    "nb_ffn_layers": 28672,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-13B",
    "release_date": "2022-06-21",
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": 20480,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-6.7B",
    "release_date": "2022-06-21",
    "nb_parameters": 6.7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 16384,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-125M",
    "release_date": "2022-06-21",
    "nb_parameters": 0.125,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "opt-2.7B",
    "release_date": "2022-06-21",
    "nb_parameters": 2.7,
    "nb_layers": 32,
    "nb_dimensions": 2560,
    "nb_heads": 32,
    "nb_ffn_layers": 10240,
    "gqa": false,
    "activation": "ReLU",
    "hardware": "A100-80GB"
  },
  {
    "model": "gpt-3-13B",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5140,
    "nb_heads": 40,
    "nb_ffn_layers": 20560,
    "gqa": false
  },
  {
    "model": "gpt-3-6.7B",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 6.7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 16384,
    "gqa": false
  },
  {
    "model": "gpt-3-2.7B",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 2.7,
    "nb_layers": 32,
    "nb_dimensions": 2560,
    "nb_heads": 32,
    "nb_ffn_layers": 10240,
    "gqa": false
  },
  {
    "model": "gpt-3-xl",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 1.3,
    "nb_layers": 24,
    "nb_dimensions": 2048,
    "nb_heads": 24,
    "nb_ffn_layers": 8192,
    "gqa": false
  },
  {
    "model": "gpt-3-small",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.125,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "gqa": false
  },
  {
    "model": "gpt-3-medium",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.35,
    "nb_layers": 24,
    "nb_dimensions": 1024,
    "nb_heads": 16,
    "nb_ffn_layers": 4096,
    "gqa": false
  },
  {
    "model": "gpt-3-large",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.76,
    "nb_layers": 24,
    "nb_dimensions": 1536,
    "nb_heads": 16,
    "nb_ffn_layers": 6144,
    "gqa": false
  },
  {
    "model": "gpt-3-175B",
    "release_date": "2020-05-28",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 175.0,
    "nb_layers": 96,
    "nb_dimensions": 12288,
    "nb_heads": 96,
    "nb_ffn_layers": 49152,
    "gqa": false
  },
  {
    "model": "gpt-2",
    "release_date": "2019-02-14",
    "context_length": 1024,
    "vocabulary_size": 50257,
    "nb_parameters": 1.5,
    "nb_layers": 48,
    "nb_dimensions": 1600,
    "nb_heads": 25,
    "nb_ffn_layers": 3072,
    "gqa": false,
    "activation": "NewGELU"
  },
  {
    "model": "bert-large",
    "release_date": "2018-10-11",
    "context_length": 512,
    "vocabulary_size": 30522,
    "nb_parameters": 0.34,
    "nb_layers": 24,
    "nb_dimensions": 1024,
    "nb_heads": 16,
    "nb_ffn_layers": 4096,
    "gqa": false,
    "activation": "GELU"
  },
  {
    "model": "bert-base",
    "release_date": "2018-10-11",
    "context_length": 512,
    "vocabulary_size": 30522,
    "nb_parameters": 0.11,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "gqa": false,
    "activation": "GELU"
  },
  {
    "model": "gpt-1",
    "release_date": "2018-06-11",
    "context_length": 512,
    "vocabulary_size": 40000,
    "nb_parameters": 0.12,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "gqa": false,
    "activation": "GELU"
  }
]