[
  {
    "model": "llama-3-8b",
    "context_length": 8192,
    "vocabulary_size": 128256,
    "nb_parameters": 8.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "nb_tokens": 15.0,
    "gqa": true
  },
  {
    "model": "llama-3-70b",
    "context_length": 8192,
    "vocabulary_size": 128256,
    "nb_parameters": 70.0,
    "nb_layers": 64,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 15.0,
    "gqa": true
  },
  {
    "model": "openelm-270M",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 0.27,
    "nb_layers": 16,
    "nb_dimensions": 1280,
    "nb_heads": 10,
    "nb_ffn_layers": -1,
    "nb_tokens": 1.8,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_hours": 9216
  },
  {
    "model": "openelm-450M",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 0.45,
    "nb_layers": 20,
    "nb_dimensions": 1536,
    "nb_heads": 12,
    "nb_ffn_layers": -1,
    "nb_tokens": 1.8,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "H100-80GB",
    "gpu_hours": 9216
  },
  {
    "model": "openelm-1.1B",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 1.1,
    "nb_layers": 28,
    "nb_dimensions": 2048,
    "nb_heads": 16,
    "nb_ffn_layers": -1,
    "nb_tokens": 1.8,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_hours": 33792
  },
  {
    "model": "openelm-3B",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 3,
    "nb_layers": 36,
    "nb_dimensions": 3072,
    "nb_heads": 24,
    "nb_ffn_layers": -1,
    "nb_tokens": 1.8,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "H100-80GB",
    "gpu_hours": 39936
  },
  {
    "model": "phi-3-mini",
    "context_length": 4096,
    "vocabulary_size": 32064,
    "nb_parameters": 3.8,
    "nb_layers": 32,
    "nb_dimensions": 3072,
    "nb_heads": 32,
    "nb_ffn_layers": 8192,
    "nb_tokens": 3.3,
    "activation": "SiLU",
    "gqa": false
  },
  {
    "model": "phi-3-mini-128K",
    "context_length": 128000,
    "vocabulary_size": 32064,
    "nb_parameters": 3.8,
    "nb_layers": 32,
    "nb_dimensions": 3072,
    "nb_heads": 32,
    "nb_ffn_layers": 8192,
    "nb_tokens": 3.3,
    "activation": "SiLU",
    "gqa": false
  },
  {
    "model": "phi-3-small",
    "context_length": 8192,
    "vocabulary_size": 100352,
    "nb_parameters": 7.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": -1,
    "nb_tokens": 4.8,
    "activation": "SiLU",
    "gqa": true
  },
  {
    "model": "phi-3-medium",
    "context_length": 8192,
    "vocabulary_size": 100352,
    "nb_parameters": 14.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": -1,
    "nb_tokens": 4.8,
    "activation": "SiLU",
    "gqa": true
  },
  {
    "model": "mixtral-8x22b",
    "context_length": 65536,
    "vocabulary_size": 32768,
    "nb_parameters": 176,
    "nb_layers": 56,
    "nb_dimensions": 6144,
    "nb_heads": 48,
    "nb_ffn_layers": 16384,
    "nb_tokens": -1,
    "activation": "SiLU",
    "gqa": true
  },
  {
    "model": "gemma-7b",
    "context_length": 8192,
    "vocabulary_size": 256000,
    "nb_parameters": 7,
    "nb_layers": 28,
    "nb_dimensions": 3072,
    "nb_heads": 16,
    "nb_ffn_layers": 49152,
    "nb_tokens": 6,
    "activation": "GeGLU",
    "gqa": false
  },
  {
    "model": "gemma-2b",
    "context_length": 8192,
    "vocabulary_size": 256000,
    "nb_parameters": 2,
    "nb_layers": 18,
    "nb_dimensions": 2048,
    "nb_heads": 8,
    "nb_ffn_layers": 32768,
    "nb_tokens": 3,
    "activation": "GeGLU",
    "gqa": false
  },
  {
    "model": "mixtral-8x7b",
    "context_length": 32768,
    "vocabulary_size": 32000,
    "nb_parameters": 56,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "nb_tokens": -1,
    "activation": "SiLU",
    "gqa": true
  },
  {
    "model": "mistral-7b",
    "context_length": 8192,
    "vocabulary_size": 32000,
    "nb_parameters": 7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 14336,
    "nb_tokens": 8,
    "activation": "SiLU",
    "gqa": false
  },
  {
    "model": "llama-2-7b",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 7.0,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 11008,
    "nb_tokens": 2.0,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 184320,
    "tCO2eq": 31.22
  },
  {
    "model": "llama-2-13b",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": 13824,
    "nb_tokens": 2.0,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 368640,
    "tCO2eq": 62.44
  },
  {
    "model": "llama-2-34b",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 34.0,
    "nb_layers": 60,
    "nb_dimensions": 6656,
    "nb_heads": 52,
    "nb_ffn_layers": 22016,
    "nb_tokens": 2.0,
    "activation": "SwiGLU",
    "gqa": true,
    "gpu_type": "A100-80GB",
    "gpu_power": 350,
    "gpu_hours": 1038336,
    "tCO2eq": 153.90
  },
  {
    "model": "llama-2-70b",
    "context_length": 4096,
    "vocabulary_size": 32000,
    "nb_parameters": 70.0,
    "nb_layers": 80,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 2.0,
    "activation": "SwiGLU",
    "gqa": true,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 1720320,
    "tCO2eq": 291.42
  },
  {
    "model": "llama-1-7b",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 6.7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 11008,
    "nb_tokens": 1.0,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 82432,
    "total_mwh": 36,
    "tCO2eq": 14
  },
  {
    "model": "llama-1-13b",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 13.0,
    "nb_layers": 40,
    "nb_dimensions": 5120,
    "nb_heads": 40,
    "nb_ffn_layers": 13824,
    "nb_tokens": 1.0,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 135168,
    "total_mwh": 59,
    "tCO2eq": 23
  },
  {
    "model": "llama-1-33b",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 32.5,
    "nb_layers": 60,
    "nb_dimensions": 6656,
    "nb_heads": 52,
    "nb_ffn_layers": 22016,
    "nb_tokens": 1.4,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 530432,
    "total_mwh": 233,
    "tCO2eq": 90
  },
  {
    "model": "llama-1-65b",
    "context_length": 2048,
    "vocabulary_size": 32000,
    "nb_parameters": 65.2,
    "nb_layers": 80,
    "nb_dimensions": 8192,
    "nb_heads": 64,
    "nb_ffn_layers": 28672,
    "nb_tokens": 1.4,
    "activation": "SwiGLU",
    "gqa": false,
    "gpu_type": "A100-80GB",
    "gpu_power": 400,
    "gpu_hours": 1022362,
    "total_mwh": 449,
    "tCO2eq": 173
  },
  {
    "model": "gpt-3-175B",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 175,
    "nb_layers": 96,
    "nb_dimensions": 12288,
    "nb_heads": 96,
    "nb_ffn_layers": 49152,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-13B",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 13,
    "nb_layers": 40,
    "nb_dimensions": 5140,
    "nb_heads": 40,
    "nb_ffn_layers": 20560,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-6.7B",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 6.7,
    "nb_layers": 32,
    "nb_dimensions": 4096,
    "nb_heads": 32,
    "nb_ffn_layers": 16384,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-2.7B",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 2.7,
    "nb_layers": 32,
    "nb_dimensions": 2560,
    "nb_heads": 32,
    "nb_ffn_layers": 10240,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-xl",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 1.3,
    "nb_layers": 24,
    "nb_dimensions": 2048,
    "nb_heads": 24,
    "nb_ffn_layers": 8192,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-large",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.76,
    "nb_layers": 24,
    "nb_dimensions": 1536,
    "nb_heads": 16,
    "nb_ffn_layers": 6144,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-medium",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.35,
    "nb_layers": 24,
    "nb_dimensions": 1024,
    "nb_heads": 16,
    "nb_ffn_layers": 4096,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-3-small",
    "context_length": 2048,
    "vocabulary_size": 50257,
    "nb_parameters": 0.125,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "nb_tokens": -1,
    "gqa": false
  },
  {
    "model": "gpt-2",
    "context_length": 1024,
    "vocabulary_size": 50257,
    "nb_parameters": 1.5,
    "nb_layers": 48,
    "nb_dimensions": 1600,
    "nb_heads": 25,
    "nb_ffn_layers": 3072,
    "nb_tokens": -1.0,
    "activation": "NewGELU",
    "gqa": false
  },
  {
    "model": "bert-base",
    "context_length": 512,
    "vocabulary_size": 30522,
    "nb_parameters": 0.11,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "nb_tokens": -1.0,
    "activation": "GELU",
    "gqa": false
  },
  {
    "model": "bert-large",
    "context_length": 512,
    "vocabulary_size": 30522,
    "nb_parameters": 0.34,
    "nb_layers": 24,
    "nb_dimensions": 1024,
    "nb_heads": 16,
    "nb_ffn_layers": 4096,
    "nb_tokens": -1.0,
    "activation": "GELU",
    "gqa": false
  },
  {
    "model": "gpt-1",
    "context_length": 512,
    "vocabulary_size": 40000,
    "nb_parameters": 0.12,
    "nb_layers": 12,
    "nb_dimensions": 768,
    "nb_heads": 12,
    "nb_ffn_layers": 3072,
    "nb_tokens": -1.0,
    "activation": "GELU",
    "gqa": false
  }
]